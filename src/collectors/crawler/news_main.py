#!/usr/bin/env python3
"""
ITWorld ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
ITWorld ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import argparse
import sys
import os
from typing import Dict, Any

# ìƒìœ„ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from news_crawler import ITWorldNewsCrawler
from news_formatter import NewsFormatter


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ITWorld ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python news_main.py --main-page                    # ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘
  python news_main.py --main-page --save-json        # JSONìœ¼ë¡œ ì €ì¥
  python news_main.py --main-page --save-csv         # CSVë¡œ ì €ì¥  
  python news_main.py --main-page --save-html        # HTML ë³´ê³ ì„œ ìƒì„±
  python news_main.py --main-page --include-content  # ë³¸ë¬¸ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘
  python news_main.py --category "https://www.itworld.co.kr/news/ai" --pages 2  # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
  python news_main.py --category "https://www.itworld.co.kr/news/ai" --include-content  # ì¹´í…Œê³ ë¦¬ + ë³¸ë¬¸
  python news_main.py --interactive                  # ëŒ€í™”í˜• ëª¨ë“œ
        """
    )
    
    # í¬ë¡¤ë§ ëª¨ë“œ
    parser.add_argument(
        '--main-page', '-m',
        action='store_true',
        help='ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§'
    )
    
    parser.add_argument(
        '--category', '-c',
        help='íŠ¹ì • ì¹´í…Œê³ ë¦¬ URL í¬ë¡¤ë§'
    )
    
    parser.add_argument(
        '--pages', '-p',
        type=int,
        default=3,
        help='ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 3)'
    )
    
    parser.add_argument(
        '--include-content', '-f',
        action='store_true',
        help='ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©(ë³¸ë¬¸)ë„ í•¨ê»˜ ìˆ˜ì§‘'
    )
    
    parser.add_argument(
        '--interactive', '-i',
        action='store_true',
        help='ëŒ€í™”í˜• ëª¨ë“œë¡œ ì‹¤í–‰'
    )
    
    # ì¶œë ¥ ì˜µì…˜
    parser.add_argument(
        '--save-json', '-j',
        action='store_true',
        help='ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥'
    )
    
    parser.add_argument(
        '--save-csv', '-v',
        action='store_true',
        help='ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥'
    )
    
    parser.add_argument(
        '--save-html', '-w',
        action='store_true',
        help='HTML ë³´ê³ ì„œ ìƒì„±'
    )
    
    parser.add_argument(
        '--detailed', '-d',
        action='store_true',
        help='ìƒì„¸í•œ ë‰´ìŠ¤ ëª©ë¡ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--max-display', '-n',
        type=int,
        default=10,
        help='í™”ë©´ì— í‘œì‹œí•  ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)'
    )
    
    # ì„¤ì • ì˜µì…˜
    parser.add_argument(
        '--timeout', '-t',
        type=int,
        default=15,
        help='HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ê°’: 15ì´ˆ)'
    )
    
    args = parser.parse_args()
    
    # í¬ë¡¤ëŸ¬ì™€ í¬ë§¤í„° ì´ˆê¸°í™”
    crawler = ITWorldNewsCrawler(timeout=args.timeout)
    formatter = NewsFormatter()
    
    if args.interactive or (not args.main_page and not args.category):
        run_interactive_mode(crawler, formatter, args)
    else:
        run_crawling_mode(crawler, formatter, args)


def run_interactive_mode(crawler: ITWorldNewsCrawler, formatter: NewsFormatter, args):
    """ëŒ€í™”í˜• ëª¨ë“œ ì‹¤í–‰"""
    print("ğŸŒ ITWorld ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ëŒ€í™”í˜• ëª¨ë“œ")
    print("=" * 60)
    print("ITWorld ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print()
    
    while True:
        try:
            print("\nğŸ“‹ í¬ë¡¤ë§ ì˜µì…˜:")
            print("  1. ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘")
            print("  2. ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘")
            print("  3. ê°œë³„ ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘")
            print("  4. ì¢…ë£Œ")
            
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-4): ").strip()
            
            if choice in ['4', 'quit', 'exit', 'q']:
                print("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            elif choice == '1':
                include_content = input("ê¸°ì‚¬ ë³¸ë¬¸ë„ í•¨ê»˜ ìˆ˜ì§‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower()
                include_content = include_content in ['y', 'yes']
                
                print("\nğŸ” ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
                if include_content:
                    print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                result = crawler.crawl_main_page_news(include_content=include_content)
                display_and_save_results(result, formatter, args)
            
            elif choice == '2':
                category_url = input("ì¹´í…Œê³ ë¦¬ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if not category_url:
                    print("âŒ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                pages = input(f"í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: {args.pages}): ").strip()
                max_pages = int(pages) if pages.isdigit() else args.pages
                
                include_content = input("ê¸°ì‚¬ ë³¸ë¬¸ë„ í•¨ê»˜ ìˆ˜ì§‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ê¸°ë³¸ê°’: n): ").strip().lower()
                include_content = include_content in ['y', 'yes']
                
                print(f"\nğŸ” ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
                if include_content:
                    print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
                result = crawler.crawl_category_news(category_url, max_pages, include_content=include_content)
                display_and_save_results(result, formatter, args)
            
            elif choice == '3':
                article_url = input("ê¸°ì‚¬ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                if not article_url:
                    print("âŒ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                print("\nğŸ“– ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì¤‘...")
                article_result = crawler.get_article_content(article_url)
                if article_result.get('success'):
                    print(f"\nì œëª©: {article_result.get('title', 'N/A')}")
                    print(f"URL: {article_result.get('url', 'N/A')}")
                    content = article_result.get('content', '')
                    if content:
                        print(f"ë‚´ìš© (ì²˜ìŒ 500ì):\n{content[:500]}...")
                        if len(content) > 500:
                            print(f"\n... (ì´ {len(content)}ì)")
                    else:
                        print("ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    print(f"âŒ ì˜¤ë¥˜: {article_result.get('error', 'Unknown error')}")
            
            else:
                print("âŒ ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                
        except KeyboardInterrupt:
            print("\n\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except EOFError:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


def run_crawling_mode(crawler: ITWorldNewsCrawler, formatter: NewsFormatter, args):
    """ëª…ë ¹í–‰ í¬ë¡¤ë§ ëª¨ë“œ ì‹¤í–‰"""
    if args.main_page:
        print("ğŸ” ITWorld ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        if args.include_content:
            print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        result = crawler.crawl_main_page_news(include_content=args.include_content)
        display_and_save_results(result, formatter, args)
    
    elif args.category:
        print(f"ğŸ” ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘: {args.category}")
        print(f"ìµœëŒ€ {args.pages}í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        if args.include_content:
            print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        result = crawler.crawl_category_news(args.category, args.pages, include_content=args.include_content)
        display_and_save_results(result, formatter, args)


def display_and_save_results(result: Dict[str, Any], formatter: NewsFormatter, args):
    """ê²°ê³¼ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not result.get('success', False):
        print(formatter._format_error_result(result))
        return
    
    # í™”ë©´ ì¶œë ¥
    if args.detailed:
        print(formatter.format_detailed_news(result, args.max_display))
    else:
        print(formatter.format_news_summary(result))
    
    # íŒŒì¼ ì €ì¥
    saved_files = []
    data_dir = formatter._get_today_data_dir()
    
    if args.save_json or args.save_csv or args.save_html:
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {data_dir}")
    
    if args.save_json:
        json_file = formatter.save_to_json(result)
        saved_files.append(f"ğŸ“ JSON: {os.path.basename(json_file)}")
    
    if args.save_csv:
        csv_file = formatter.save_to_csv(result)
        if csv_file:
            saved_files.append(f"ğŸ“Š CSV: {os.path.basename(csv_file)}")
    
    if args.save_html:
        html_file = formatter.create_html_report(result)
        saved_files.append(f"ğŸŒ HTML: {os.path.basename(html_file)}")
    
    if saved_files:
        print("ğŸ“‹ ì €ì¥ëœ íŒŒì¼:")
        for file_info in saved_files:
            print(f"  {file_info}")
        
        # ë””ë ‰í„°ë¦¬ êµ¬ì¡° ì•ˆë‚´
        print(f"\nğŸ“‚ ì „ì²´ ê²½ë¡œ: {data_dir}")
        print("ğŸ’¡ ë™ì¼í•œ ë‚ ì§œì˜ ê¸°ì¡´ íŒŒì¼ë“¤ì€ ìë™ìœ¼ë¡œ ì‚­ì œë˜ê³  ìƒˆë¡œ ì €ì¥ë©ë‹ˆë‹¤.")


def show_categories():
    """ì£¼ìš” ITWorld ì¹´í…Œê³ ë¦¬ URLë“¤ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."""
    categories = {
        "ì¸ê³µì§€ëŠ¥": "https://www.itworld.co.kr/news/ai",
        "í´ë¼ìš°ë“œ": "https://www.itworld.co.kr/news/cloud",
        "ë³´ì•ˆ": "https://www.itworld.co.kr/news/security",
        "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ": "https://www.itworld.co.kr/news/development",
        "ëª¨ë°”ì¼": "https://www.itworld.co.kr/news/mobile",
        "ë„¤íŠ¸ì›Œí¬": "https://www.itworld.co.kr/news/network",
        "ë°ì´í„°ì„¼í„°": "https://www.itworld.co.kr/news/datacenter"
    }
    
    print("\nğŸ“‚ ì£¼ìš” ITWorld ì¹´í…Œê³ ë¦¬:")
    for name, url in categories.items():
        print(f"  â€¢ {name}: {url}")


if __name__ == "__main__":
    main() 