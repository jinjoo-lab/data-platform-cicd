import requests
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse
import re
from typing import Dict, List, Any, Optional
import time


class WebAnalyzer:
    """ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, timeout: int = 10, max_retries: int = 3):
        """
        WebAnalyzer ì´ˆê¸°í™”
        
        Args:
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def crawl_website(self, url: str) -> Dict[str, Any]:
        """
        ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            url: í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URL
            
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # URL ì •ê·œí™”
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            print(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
            
            # HTTP ìš”ì²­
            response = self._make_request(url)
            if not response:
                return self._create_error_result(url, "ì›¹ì‚¬ì´íŠ¸ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì›¹ì‚¬ì´íŠ¸ ë¶„ì„
            analysis_result = {
                'url': url,
                'status_code': response.status_code,
                'success': True,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'basic_info': self._extract_basic_info(soup),
                'content_analysis': self._analyze_content(soup),
                'structure_analysis': self._analyze_structure(soup),
                'data_opportunities': self._identify_data_opportunities(soup)
            }
            
            return analysis_result
            
        except Exception as e:
            return self._create_error_result(url, f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _make_request(self, url: str) -> Optional[requests.Response]:
        """HTTP ìš”ì²­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                print(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
        return None
    
    def _extract_basic_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """ì›¹ì‚¬ì´íŠ¸ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        title = soup.find('title')
        description = soup.find('meta', attrs={'name': 'description'})
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        
        return {
            'title': title.text.strip() if title else 'No title found',
            'description': description.get('content', 'No description found') if description else 'No description found',
            'keywords': keywords.get('content', 'No keywords found') if keywords else 'No keywords found'
        }
    
    def _analyze_content(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì›¹ì‚¬ì´íŠ¸ì˜ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ë¶„ì„
        text_content = soup.get_text()
        words = re.findall(r'\w+', text_content.lower())
        
        # ì´ë¯¸ì§€ ë¶„ì„
        images = soup.find_all('img')
        
        # ë§í¬ ë¶„ì„
        links = soup.find_all('a', href=True)
        
        return {
            'text_stats': {
                'total_words': len(words),
                'unique_words': len(set(words)),
                'most_common_words': self._get_most_common_words(words, 10)
            },
            'media_stats': {
                'total_images': len(images),
                'images_with_alt': len([img for img in images if img.get('alt')])
            },
            'link_stats': {
                'total_links': len(links),
                'internal_links': len([link for link in links if self._is_internal_link(link.get('href'))]),
                'external_links': len([link for link in links if not self._is_internal_link(link.get('href'))])
            }
        }
    
    def _analyze_structure(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ì›¹ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        # HTML íƒœê·¸ ë¶„ì„
        all_tags = soup.find_all()
        tag_counts = {}
        for tag in all_tags:
            tag_name = tag.name
            tag_counts[tag_name] = tag_counts.get(tag_name, 0) + 1
        
        # í…Œì´ë¸” ë¶„ì„
        tables = soup.find_all('table')
        
        # í¼ ë¶„ì„
        forms = soup.find_all('form')
        
        # ë¦¬ìŠ¤íŠ¸ ë¶„ì„
        lists = soup.find_all(['ul', 'ol'])
        
        return {
            'tag_distribution': dict(sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:15]),
            'structural_elements': {
                'tables': len(tables),
                'forms': len(forms),
                'lists': len(lists),
                'headings': len(soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])),
                'paragraphs': len(soup.find_all('p'))
            }
        }
    
    def _identify_data_opportunities(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ê°€ëŠ¥í•œ ë°ì´í„° ê¸°íšŒë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        opportunities = []
        
        # í…Œì´ë¸” ë°ì´í„°
        tables = soup.find_all('table')
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            cols = len(table.find_all('th')) or len(table.find_all('td'))
            if rows and cols:
                opportunities.append({
                    'type': 'table',
                    'description': f'í…Œì´ë¸” #{i+1} ({len(rows)}í–‰ x {cols}ì—´)',
                    'potential_data': 'êµ¬ì¡°í™”ëœ í‘œ ë°ì´í„°'
                })
        
        # ë¦¬ìŠ¤íŠ¸ ë°ì´í„°
        lists = soup.find_all(['ul', 'ol'])
        for i, list_elem in enumerate(lists):
            items = list_elem.find_all('li')
            if len(items) > 3:  # ì˜ë¯¸ìˆëŠ” ë¦¬ìŠ¤íŠ¸ë§Œ
                opportunities.append({
                    'type': 'list',
                    'description': f'ë¦¬ìŠ¤íŠ¸ #{i+1} ({len(items)}ê°œ í•­ëª©)',
                    'potential_data': 'ëª©ë¡ í˜•íƒœì˜ ë°ì´í„°'
                })
        
        # ë°˜ë³µë˜ëŠ” íŒ¨í„´ (ì˜ˆ: ì œí’ˆ, ê¸°ì‚¬ ë“±)
        common_classes = self._find_repeated_patterns(soup)
        for class_name, count in common_classes.items():
            if count > 3:  # 3ê°œ ì´ìƒ ë°˜ë³µë˜ëŠ” íŒ¨í„´
                opportunities.append({
                    'type': 'repeated_pattern',
                    'description': f'ë°˜ë³µ íŒ¨í„´: .{class_name} ({count}ê°œ)',
                    'potential_data': 'ë°˜ë³µë˜ëŠ” êµ¬ì¡°í™”ëœ ì½˜í…ì¸ '
                })
        
        # í¼ ë°ì´í„°
        forms = soup.find_all('form')
        for i, form in enumerate(forms):
            inputs = form.find_all(['input', 'select', 'textarea'])
            if inputs:
                opportunities.append({
                    'type': 'form',
                    'description': f'í¼ #{i+1} ({len(inputs)}ê°œ ì…ë ¥ í•„ë“œ)',
                    'potential_data': 'ì‚¬ìš©ì ì…ë ¥ ë°ì´í„° êµ¬ì¡°'
                })
        
        return {
            'total_opportunities': len(opportunities),
            'opportunities': opportunities,
            'recommendations': self._generate_recommendations(opportunities)
        }
    
    def _get_most_common_words(self, words: List[str], limit: int) -> List[tuple]:
        """ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        word_counts = {}
        for word in words:
            if len(word) > 2:  # 2ê¸€ì ì´ìƒë§Œ
                word_counts[word] = word_counts.get(word, 0) + 1
        
        return sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:limit]
    
    def _is_internal_link(self, href: str) -> bool:
        """ë‚´ë¶€ ë§í¬ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
        if not href:
            return False
        return href.startswith('/') or href.startswith('#') or not href.startswith('http')
    
    def _find_repeated_patterns(self, soup: BeautifulSoup) -> Dict[str, int]:
        """ë°˜ë³µë˜ëŠ” CSS í´ë˜ìŠ¤ íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤."""
        class_counts = {}
        for elem in soup.find_all(class_=True):
            for class_name in elem.get('class', []):
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
        
        return {k: v for k, v in class_counts.items() if v > 1}
    
    def _generate_recommendations(self, opportunities: List[Dict]) -> List[str]:
        """í¬ë¡¤ë§ ì¶”ì²œì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []
        
        table_count = len([o for o in opportunities if o['type'] == 'table'])
        list_count = len([o for o in opportunities if o['type'] == 'list'])
        pattern_count = len([o for o in opportunities if o['type'] == 'repeated_pattern'])
        
        if table_count > 0:
            recommendations.append(f"ğŸ“Š {table_count}ê°œì˜ í…Œì´ë¸”ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œì— ì í•©í•©ë‹ˆë‹¤.")
        
        if list_count > 0:
            recommendations.append(f"ğŸ“ {list_count}ê°œì˜ ë¦¬ìŠ¤íŠ¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ëª©ë¡ ë°ì´í„° ìˆ˜ì§‘ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        if pattern_count > 0:
            recommendations.append(f"ğŸ”„ {pattern_count}ê°œì˜ ë°˜ë³µ íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ì— ìœ ë¦¬í•©ë‹ˆë‹¤.")
        
        if not opportunities:
            recommendations.append("âš ï¸ íŠ¹ë³„í•œ ë°ì´í„° íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        
        return recommendations
    
    def _create_error_result(self, url: str, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            'url': url,
            'success': False,
            'error': error_message,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        } 