import requests
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional
from datetime import datetime


class ITWorldNewsCrawler:
    """ITWorld ì›¹ì‚¬ì´íŠ¸ ì „ìš© ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, timeout: int = 15, max_retries: int = 3):
        """
        ITWorldNewsCrawler ì´ˆê¸°í™”
        
        Args:
            timeout: HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        """
        self.base_url = "https://www.itworld.co.kr"
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Referer': 'https://www.itworld.co.kr/'
        })
    
    def crawl_main_page_news(self, include_content: bool = False) -> Dict[str, Any]:
        """
        ë©”ì¸ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ëª©ë¡ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            include_content: Trueì´ë©´ ê° ê¸°ì‚¬ì˜ ë³¸ë¬¸ë„ í•¨ê»˜ ìˆ˜ì§‘
        
        Returns:
            ë‰´ìŠ¤ ëª©ë¡ì´ í¬í•¨ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            print("ğŸ” ITWorld ë©”ì¸ í˜ì´ì§€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
            if include_content:
                print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            
            response = self._make_request(self.base_url)
            if not response:
                return self._create_error_result("ë©”ì¸ í˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì „ì²´ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ë“¤ì„ ë¨¼ì € ìˆ˜ì§‘
            article_links = {}
            all_article_links = soup.find_all('a', href=lambda x: x and '/article/' in x)
            print(f"ğŸ”— ë°œê²¬ëœ ê¸°ì‚¬ ë§í¬: {len(all_article_links)}ê°œ")
            
            for link in all_article_links:
                title = link.get_text(strip=True)
                href = link.get('href')
                if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                    # ì œëª©ì„ ì •ë¦¬í•´ì„œ ë§¤ì¹­ í‚¤ë¡œ ì‚¬ìš©
                    clean_title = self._clean_title_for_matching(title)
                    article_links[clean_title] = urljoin(self.base_url, href)
            
            print(f"ğŸ“ ë§¤ì¹­ìš© ì œëª©-ë§í¬ ë§¤í•‘: {len(article_links)}ê°œ")
            
            # ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘
            news_list = []
            
            # ë©”ì¸ ë‰´ìŠ¤ ì¹´ë“œë“¤ ì¶”ì¶œ
            main_cards = soup.find_all('div', class_='card')
            print(f"ğŸ“° ë°œê²¬ëœ ë‰´ìŠ¤ ì¹´ë“œ: {len(main_cards)}ê°œ")
            
            for i, card in enumerate(main_cards):
                try:
                    news_item = self._extract_news_from_card(card, article_links)
                    if news_item:
                        news_list.append(news_item)
                        print(f"  âœ… ë‰´ìŠ¤ {i+1}: {news_item.get('title', 'N/A')[:50]}...")
                except Exception as e:
                    print(f"  âŒ ì¹´ë“œ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # ì¶”ê°€ ë‰´ìŠ¤ ì„¹ì…˜ë“¤ í™•ì¸
            content_rows = soup.find_all('div', class_='content-row-article')
            print(f"ğŸ“‘ ì¶”ê°€ ì»¨í…ì¸  í–‰: {len(content_rows)}ê°œ")
            
            for row in content_rows:
                try:
                    # ë©”ì¸ ë‰´ìŠ¤
                    main_article = row.find('div', class_='content-row-article__main')
                    if main_article:
                        main_card = main_article.find('div', class_='card')
                        if main_card:
                            news_item = self._extract_news_from_card(main_card, article_links)
                            if news_item and not any(n['title'] == news_item['title'] for n in news_list):
                                news_list.append(news_item)
                    
                    # ì‚¬ì´ë“œ ë‰´ìŠ¤ë“¤
                    secondary_section = row.find('div', class_='content-row-article__secondary')
                    if secondary_section:
                        secondary_cards = secondary_section.find_all('div', class_='card')
                        for card in secondary_cards:
                            news_item = self._extract_news_from_card(card, article_links)
                            if news_item and not any(n['title'] == news_item['title'] for n in news_list):
                                news_list.append(news_item)
                                
                except Exception as e:
                    print(f"  âŒ ì»¨í…ì¸  í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            unique_news = []
            seen_titles = set()
            for news in news_list:
                if news['title'] not in seen_titles:
                    unique_news.append(news)
                    seen_titles.add(news['title'])
            
            # ì„¸ë¶€ ë‚´ìš© ìˆ˜ì§‘ (ì˜µì…˜)
            if include_content:
                unique_news = self._collect_article_contents(unique_news)
            
            result = {
                'success': True,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'ITWorld ë©”ì¸ í˜ì´ì§€',
                'url': self.base_url,
                'total_news': len(unique_news),
                'news_list': unique_news,
                'categories': self._extract_categories(unique_news),
                'summary': self._generate_summary(unique_news),
                'content_included': include_content
            }
            
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(unique_news)}ê°œì˜ ê³ ìœ  ë‰´ìŠ¤ ìˆ˜ì§‘")
            if include_content:
                content_count = len([n for n in unique_news if n.get('full_content')])
                print(f"ğŸ“– ì„¸ë¶€ ë‚´ìš© ìˆ˜ì§‘: {content_count}ê°œ ê¸°ì‚¬")
            
            return result
            
        except Exception as e:
            return self._create_error_result(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _clean_title_for_matching(self, title: str) -> str:
        """ì œëª©ì„ ë§¤ì¹­ì„ ìœ„í•´ ì •ë¦¬í•©ë‹ˆë‹¤."""
        import re
        # ë¶ˆí•„ìš”í•œ ë¬¸ìë“¤ ì œê±°
        clean = re.sub(r'[^\w\sê°€-í£]', '', title.lower())
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        clean = re.sub(r'\s+', ' ', clean).strip()
        # ì²˜ìŒ 50ìë§Œ ì‚¬ìš© (ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ)
        return clean[:50]
    
    def _extract_news_from_card(self, card, article_links: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """ì¹´ë“œì—ì„œ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ë§í¬ë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤."""
        try:
            news_item = {}
            
            # ì œëª© ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            title_elem = card.find('h3', class_='card__title') or card.find('div', class_='card__title')
            if title_elem:
                news_item['title'] = title_elem.get_text(strip=True)
            
            # ì œëª©ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì°¾ê¸°
            if not news_item.get('title'):
                # h2, h3 íƒœê·¸ì—ì„œ ì œëª© ì°¾ê¸°
                for header_tag in ['h2', 'h3', 'h4']:
                    header = card.find(header_tag)
                    if header:
                        title_text = header.get_text(strip=True)
                        if len(title_text) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                            news_item['title'] = title_text
                            break
            
            if not news_item.get('title'):
                return None
            
            # ì œëª©ì„ ì´ìš©í•´ ë§í¬ ë§¤ì¹­
            clean_title = self._clean_title_for_matching(news_item['title'])
            
            # ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” ë§í¬ ì°¾ê¸°
            matched_url = None
            for link_title, url in article_links.items():
                if clean_title == link_title:
                    matched_url = url
                    break
            
            # ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            if not matched_url:
                title_words = clean_title.split()
                if len(title_words) >= 3:  # ìµœì†Œ 3ë‹¨ì–´ ì´ìƒì¼ ë•Œë§Œ
                    for link_title, url in article_links.items():
                        # ì•ì˜ 3ë‹¨ì–´ê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if all(word in link_title for word in title_words[:3]):
                            matched_url = url
                            break
            
            if matched_url:
                news_item['url'] = matched_url
                print(f"    ğŸ”— ë§í¬ ë§¤ì¹­ ì„±ê³µ: {news_item['title'][:30]}...")
            else:
                print(f"    âš ï¸  ë§í¬ ë§¤ì¹­ ì‹¤íŒ¨: {news_item['title'][:30]}...")
            
            # ì½˜í…ì¸  íƒ€ì… ì¶”ì¶œ
            content_type_selectors = [
                'span.card__content-type',
                'div.card__content-type', 
                '.content-type',
                '.post-type'
            ]
            for selector in content_type_selectors:
                content_type = card.select_one(selector)
                if content_type:
                    news_item['content_type'] = content_type.get_text(strip=True)
                    break
            
            # ì„¤ëª… ì¶”ì¶œ
            description_selectors = [
                'p.card__description',
                'div.card__description',
                '.description',
                '.excerpt',
                'p'
            ]
            for selector in description_selectors:
                description = card.select_one(selector)
                if description:
                    desc_text = description.get_text(strip=True)
                    if len(desc_text) > 20:  # ì˜ë¯¸ìˆëŠ” ì„¤ëª…ë§Œ
                        news_item['description'] = desc_text
                        break
            
            # íƒœê·¸ ì¶”ì¶œ
            tags = []
            tags_selectors = [
                'div.card__tags span.tag',
                '.tags .tag',
                '.category',
                '.topic'
            ]
            for selector in tags_selectors:
                tag_elements = card.select(selector)
                if tag_elements:
                    tags = [tag.get_text(strip=True) for tag in tag_elements if tag.get_text(strip=True)]
                    break
            news_item['tags'] = tags
            
            # ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            info_elements = card.find_all(['div', 'span'], class_=lambda x: x and 'info' in x)
            for info in info_elements:
                info_text = info.get_text(strip=True)
                # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', info_text)
                if date_match:
                    news_item['publish_date'] = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                
                # ì‹œê°„ íŒ¨í„´ ì°¾ê¸°
                time_match = re.search(r'(\d{1,2}ë¶„)', info_text)
                if time_match:
                    news_item['read_time'] = time_match.group(1)
                
                # ì‘ì„±ì ì •ë³´
                if 'By' in info_text:
                    author_match = re.search(r'By\s+(.+?)(?:\s|$)', info_text)
                    if author_match:
                        news_item['author'] = author_match.group(1).strip()
            
            # ì „ì²´ ì¹´ë“œ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œì™€ ì‘ì„±ì ì°¾ê¸°
            if not news_item.get('publish_date') or not news_item.get('author'):
                card_text = card.get_text()
                
                # ë‚ ì§œ ì°¾ê¸°
                if not news_item.get('publish_date'):
                    date_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', card_text)
                    if date_match:
                        news_item['publish_date'] = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                
                # ì‘ì„±ì ì°¾ê¸°
                if not news_item.get('author'):
                    author_match = re.search(r'By\s+(.+?)(?:\s|$)', card_text)
                    if author_match:
                        news_item['author'] = author_match.group(1).strip()
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            image_elem = card.find('img')
            if image_elem:
                image_src = image_elem.get('src') or image_elem.get('data-src')
                if image_src:
                    news_item['image_url'] = urljoin(self.base_url, image_src)
                    news_item['image_alt'] = image_elem.get('alt', '')
            
            # ë©”íƒ€ë°ì´í„°
            news_item['crawled_at'] = datetime.now().isoformat()
            news_item['source'] = 'ITWorld'
            
            print(f"    âœ… ì¶”ì¶œ ì„±ê³µ: {news_item.get('title', 'No title')[:50]}...")
            return news_item
            
        except Exception as e:
            print(f"    âŒ ì¹´ë“œì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _collect_article_contents(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ë‰´ìŠ¤ ëª©ë¡ì˜ ê° ê¸°ì‚¬ì— ëŒ€í•´ ì„¸ë¶€ ë‚´ìš©ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            news_list: ë‰´ìŠ¤ ëª©ë¡
            
        Returns:
            ì„¸ë¶€ ë‚´ìš©ì´ ì¶”ê°€ëœ ë‰´ìŠ¤ ëª©ë¡
        """
        print(f"\nğŸ“– {len(news_list)}ê°œ ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš© ìˆ˜ì§‘ ì¤‘...")
        
        for i, news in enumerate(news_list):
            if not news.get('url'):
                print(f"  âš ï¸  ê¸°ì‚¬ {i+1}: URLì´ ì—†ì–´ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            try:
                print(f"  ğŸ“„ ê¸°ì‚¬ {i+1}/{len(news_list)}: {news.get('title', 'N/A')[:40]}...")
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                article_content = self.get_article_content(news['url'])
                
                if article_content.get('success'):
                    # ë³¸ë¬¸ ë‚´ìš© ì¶”ê°€
                    news['full_content'] = article_content.get('content', '')
                    news['content_length'] = len(news['full_content'])
                    
                    # ì¶”ê°€ë¡œ ì–»ì€ ì •ë³´ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                    if article_content.get('author') and not news.get('author'):
                        news['author'] = article_content.get('author')
                    
                    if article_content.get('publish_date') and not news.get('publish_date'):
                        news['publish_date'] = article_content.get('publish_date')
                    
                    print(f"    âœ… ì„±ê³µ ({len(news['full_content'])}ì)")
                else:
                    print(f"    âŒ ì‹¤íŒ¨: {article_content.get('error', 'Unknown error')}")
                    news['full_content'] = ''
                    news['content_length'] = 0
                
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
                
            except Exception as e:
                print(f"    âŒ ê¸°ì‚¬ {i+1} ë‚´ìš© ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                news['full_content'] = ''
                news['content_length'] = 0
                continue
        
        return news_list

    def crawl_category_news(self, category_url: str, max_pages: int = 3, include_content: bool = False) -> Dict[str, Any]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        
        Args:
            category_url: ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ URL
            max_pages: ìµœëŒ€ í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜
            include_content: Trueì´ë©´ ê° ê¸°ì‚¬ì˜ ë³¸ë¬¸ë„ í•¨ê»˜ ìˆ˜ì§‘
            
        Returns:
            ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ëª©ë¡
        """
        try:
            print(f"ğŸ” ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘: {category_url}")
            if include_content:
                print("ğŸ“– ê° ê¸°ì‚¬ì˜ ì„¸ë¶€ ë‚´ìš©ë„ í•¨ê»˜ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            
            all_news = []
            
            for page in range(1, max_pages + 1):
                page_url = f"{category_url}?page={page}" if page > 1 else category_url
                
                response = self._make_request(page_url)
                if not response:
                    print(f"  âŒ í˜ì´ì§€ {page} ì ‘ê·¼ ì‹¤íŒ¨")
                    break
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ë“¤ ìˆ˜ì§‘
                article_links = {}
                page_article_links = soup.find_all('a', href=lambda x: x and '/article/' in x)
                
                for link in page_article_links:
                    title = link.get_text(strip=True)
                    href = link.get('href')
                    if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                        clean_title = self._clean_title_for_matching(title)
                        article_links[clean_title] = urljoin(self.base_url, href)
                
                cards = soup.find_all('div', class_='card')
                
                if not cards:
                    print(f"  âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                print(f"  ğŸ“„ í˜ì´ì§€ {page}: {len(cards)}ê°œ ë‰´ìŠ¤ ë°œê²¬, {len(article_links)}ê°œ ë§í¬ ë§¤í•‘")
                
                for card in cards:
                    news_item = self._extract_news_from_card(card, article_links)
                    if news_item:
                        all_news.append(news_item)
                
                # í˜ì´ì§€ ê°„ ìš”ì²­ ê°„ê²©
                time.sleep(1)
            
            # ì¤‘ë³µ ì œê±°
            unique_news = []
            seen_titles = set()
            for news in all_news:
                if news['title'] not in seen_titles:
                    unique_news.append(news)
                    seen_titles.add(news['title'])
            
            # ì„¸ë¶€ ë‚´ìš© ìˆ˜ì§‘ (ì˜µì…˜)
            if include_content:
                unique_news = self._collect_article_contents(unique_news)
            
            result = {
                'success': True,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'source': f'ITWorld ì¹´í…Œê³ ë¦¬',
                'url': category_url,
                'pages_crawled': page,
                'total_news': len(unique_news),
                'news_list': unique_news,
                'categories': self._extract_categories(unique_news),
                'summary': self._generate_summary(unique_news),
                'content_included': include_content
            }
            
            print(f"âœ… ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì™„ë£Œ: {len(unique_news)}ê°œì˜ ê³ ìœ  ë‰´ìŠ¤ ìˆ˜ì§‘")
            if include_content:
                content_count = len([n for n in unique_news if n.get('full_content')])
                print(f"ğŸ“– ì„¸ë¶€ ë‚´ìš© ìˆ˜ì§‘: {content_count}ê°œ ê¸°ì‚¬")
            
            return result
            
        except Exception as e:
            return self._create_error_result(f"ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def get_article_content(self, article_url: str) -> Dict[str, Any]:
        """
        ê°œë³„ ê¸°ì‚¬ì˜ ì „ì²´ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            article_url: ê¸°ì‚¬ URL
            
        Returns:
            ê¸°ì‚¬ ì „ì²´ ë‚´ìš©
        """
        try:
            response = self._make_request(article_url)
            if not response:
                return self._create_error_result("ê¸°ì‚¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ (ITWorld íŠ¹í™”)
            article_content = {
                'url': article_url,
                'title': '',
                'content': '',
                'publish_date': '',
                'author': '',
                'tags': [],
                'category': ''
            }
            
            # ì œëª© ì¶”ì¶œ
            title_selectors = [
                'h1.article-title',
                'h1.post-title', 
                '.article-header h1',
                '.content-header h1',
                'h1',
                '.page-title h1'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    article_content['title'] = title_elem.get_text(strip=True)
                    break
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ITWorld íŠ¹í™” ì„ íƒìë“¤)
            content_selectors = [
                '.article-content',
                '.post-content', 
                '.content-body',
                '.article-body',
                '.story-body',
                '[data-module="ArticleBody"]',
                '.entry-content',
                'article .content',
                '.main-content'
            ]
            
            content_found = False
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for unwanted in content_elem.find_all([
                        'script', 'style', 'aside', 'nav', 'footer', 
                        '.advertisement', '.ad', '.social-share',
                        '.related-articles', '.comments'
                    ]):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    paragraphs = content_elem.find_all(['p', 'div'], recursive=True)
                    content_parts = []
                    
                    for p in paragraphs:
                        text = p.get_text(separator=' ', strip=True)
                        # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ (ê¸¸ì´ê°€ 20ì ì´ìƒì´ê³  ê´‘ê³ ì„± í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ)
                        if (len(text) > 20 and 
                            not any(ad_word in text.lower() for ad_word in 
                                   ['advertisement', 'ê´‘ê³ ', 'sponsored', 'í›„ì›'])):
                            content_parts.append(text)
                    
                    if content_parts:
                        article_content['content'] = '\n\n'.join(content_parts)
                        content_found = True
                        break
            
            # ê¸°ë³¸ ë¬¸ë‹¨ ì¶”ì¶œ (ìœ„ ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°)
            if not content_found:
                main_content = soup.find('main') or soup.find('article') or soup
                paragraphs = main_content.find_all('p')
                content_parts = []
                
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if len(text) > 50:  # ë” ê¸´ ë¬¸ë‹¨ë§Œ
                        content_parts.append(text)
                
                if content_parts:
                    article_content['content'] = '\n\n'.join(content_parts[:10])  # ìµœëŒ€ 10ê°œ ë¬¸ë‹¨
            
            # ì‘ì„±ì ì¶”ì¶œ
            author_selectors = [
                '.article-author',
                '.author-name',
                '.byline .author',
                '.post-author',
                '[rel="author"]'
            ]
            
            for selector in author_selectors:
                author_elem = soup.select_one(selector)
                if author_elem:
                    article_content['author'] = author_elem.get_text(strip=True)
                    break
            
            # ë‚ ì§œ ì¶”ì¶œ
            date_selectors = [
                '.article-date',
                '.publish-date', 
                '.post-date',
                'time[datetime]',
                '.date-published'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get('datetime') or date_elem.get_text(strip=True)
                    # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                    date_match = re.search(r'(\d{4})[-./](\d{1,2})[-./](\d{1,2})', date_text)
                    if date_match:
                        article_content['publish_date'] = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
                        break
            
            # ì¹´í…Œê³ ë¦¬/íƒœê·¸ ì¶”ì¶œ
            tag_selectors = [
                '.article-tags a',
                '.post-categories a',
                '.tags a',
                '.category a'
            ]
            
            for selector in tag_selectors:
                tag_elements = soup.select(selector)
                if tag_elements:
                    article_content['tags'] = [tag.get_text(strip=True) for tag in tag_elements]
                    break
            
            article_content['crawled_at'] = datetime.now().isoformat()
            article_content['success'] = True
            
            return article_content
            
        except Exception as e:
            return self._create_error_result(f"ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    def _extract_categories(self, news_list: List[Dict]) -> Dict[str, int]:
        """ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ì¹´í…Œê³ ë¦¬ í†µê³„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        categories = {}
        for news in news_list:
            for tag in news.get('tags', []):
                categories[tag] = categories.get(tag, 0) + 1
        return dict(sorted(categories.items(), key=lambda x: x[1], reverse=True))
    
    def _generate_summary(self, news_list: List[Dict]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ëª©ë¡ì˜ ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not news_list:
            return {}
        
        # ë‚ ì§œë³„ ë¶„í¬
        dates = {}
        for news in news_list:
            date = news.get('publish_date', 'Unknown')
            dates[date] = dates.get(date, 0) + 1
        
        # ì»¨í…ì¸  íƒ€ì…ë³„ ë¶„í¬
        content_types = {}
        for news in news_list:
            content_type = news.get('content_type', 'Unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        return {
            'total_articles': len(news_list),
            'date_distribution': dates,
            'content_type_distribution': content_types,
            'latest_date': max([n.get('publish_date', '') for n in news_list if n.get('publish_date')], default=''),
            'has_images': len([n for n in news_list if n.get('image_url')])
        }
    
    def _make_request(self, url: str) -> Optional[requests.Response]:
        """HTTP ìš”ì²­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                print(f"  âš ï¸ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries}): {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
        return None
    
    def _create_error_result(self, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return {
            'success': False,
            'error': error_message,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'news_list': []
        } 