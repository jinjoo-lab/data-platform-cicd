#!/usr/bin/env python3
"""
ğŸ³ PySpark í´ëŸ¬ìŠ¤í„° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Docker Compose Spark í´ëŸ¬ìŠ¤í„°ì™€ ì—°ë™í•˜ì—¬ ITWorld ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
Jupyter Lab ë…¸íŠ¸ë¶ì´ ë¬¸ì œê°€ ìˆì„ ë•Œ ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime

def create_spark_session(use_cluster=True):
    """Spark ì„¸ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # Docker Compose Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì„¤ì •
    SPARK_MASTER_URL = "spark://localhost:7077"
    
    if use_cluster:
        print(f"ğŸ³ Docker Compose Spark í´ëŸ¬ìŠ¤í„°ì— ì—°ê²° ì¤‘...")
        print(f"Master URL: {SPARK_MASTER_URL}")
        
        # í´ëŸ¬ìŠ¤í„° ëª¨ë“œë¡œ Spark ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .appName("ITWorld News Analysis - Cluster") \
            .master(SPARK_MASTER_URL) \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.driver.memory", "2g") \
            .config("spark.executor.memory", "1g") \
            .config("spark.executor.cores", "2") \
            .getOrCreate()
    else:
        print(f"ğŸ’» ë¡œì»¬ Spark ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
        
        # ë¡œì»¬ ëª¨ë“œë¡œ Spark ì„¸ì…˜ ìƒì„±
        spark = SparkSession.builder \
            .appName("ITWorld News Analysis - Local") \
            .master("local[*]") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()

    print(f"\nğŸš€ Spark ì„¸ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"Spark ë²„ì „: {spark.version}")
    print(f"Spark UI: {spark.sparkContext.uiWebUrl}")
    print(f"ì‹¤í–‰ ëª¨ë“œ: {'ğŸ³ í´ëŸ¬ìŠ¤í„°' if use_cluster else 'ğŸ’» ë¡œì»¬'}")
    
    return spark

def load_itworld_data(spark):
    """ITWorld ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    data_base_path = "../data"
    
    try:
        itworld_dirs = [d for d in os.listdir(data_base_path) if d.startswith("ITWorld_")]
        
        if itworld_dirs:
            latest_dir = sorted(itworld_dirs)[-1]
            data_path = os.path.join(data_base_path, latest_dir)
            print(f"ğŸ“ ë°ì´í„° ë””ë ‰í„°ë¦¬: {data_path}")
            
            # JSON íŒŒì¼ ì°¾ê¸°
            json_files = [f for f in os.listdir(data_path) if f.endswith('.json')]
            
            if json_files:
                json_file = os.path.join(data_path, json_files[0])
                print(f"ğŸ“„ JSON íŒŒì¼: {json_file}")
                
                # Spark DataFrameìœ¼ë¡œ ë¡œë“œ
                df = spark.read.option("multiline", "true").json(json_file)
                
                print(f"\nğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
                print(f"ì´ {df.count()}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬")
                print(f"\nğŸ“‹ ë°ì´í„° ìŠ¤í‚¤ë§ˆ:")
                df.printSchema()
                
                return df
                
            else:
                print("âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        else:
            print("âŒ ITWorld ë°ì´í„° ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”:")
            print("   cd collectors/crawler && python news_main.py --main-page -f")
            return None
            
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ’¡ data ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None

def analyze_data(df):
    """ê¸°ë³¸ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    
    if df is None:
        print("âš ï¸ ë°ì´í„°ê°€ ì—†ì–´ì„œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*50)
    print("ğŸ“ˆ ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì‹œì‘")
    print("="*50)
    
    # 1. ë°ì´í„° ìƒ˜í”Œ ë³´ê¸°
    print("\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):")
    df.select("title", "description", "category", "published_date").show(5, truncate=False)
    
    # 2. ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜
    print("\nğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜:")
    category_counts = df.groupBy("category").count().orderBy(desc("count"))
    category_counts.show()
    
    # 3. í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
    print("\nğŸ“Š í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„:")
    df_with_stats = df.withColumn("title_length", length(col("title"))) \
                      .withColumn("desc_length", length(col("description")))
    
    df_with_stats.select("title_length", "desc_length").describe().show()
    
    # 4. í‚¤ì›Œë“œ ë¶„ì„
    print("\nğŸ” ì œëª©ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œ:")
    words_df = df.select(explode(split(col("title"), "\\s+")).alias("word")) \
                  .filter(length(col("word")) >= 3) \
                  .groupBy("word") \
                  .count() \
                  .orderBy(desc("count"))
    
    print("ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ:")
    words_df.show(10, truncate=False)
    
    return category_counts, df_with_stats

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ³ PySpark í´ëŸ¬ìŠ¤í„° í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print("=" * 60)
    
    # ğŸš€ 1. Spark ì„¸ì…˜ ìƒì„±
    USE_CLUSTER = True  # Falseë¡œ ë³€ê²½í•˜ë©´ ë¡œì»¬ ëª¨ë“œ
    spark = create_spark_session(USE_CLUSTER)
    
    # ğŸ“ 2. ë°ì´í„° ë¡œë“œ
    df = load_itworld_data(spark)
    
    # ğŸ“ˆ 3. ë°ì´í„° ë¶„ì„
    if df is not None:
        analyze_data(df)
    
    # ğŸ›‘ 4. ì„¸ì…˜ ì¢…ë£Œ ì•ˆë‚´
    print("\n" + "="*60)
    print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print("\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. ğŸ³ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§: http://localhost:8080")
    print("2. ğŸ“Š ì½”ë“œë¥¼ ìˆ˜ì •í•´ì„œ ì¶”ê°€ ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”")
    print("3. ğŸ”„ spark.stop()ìœ¼ë¡œ ì„¸ì…˜ì„ ì¢…ë£Œí•˜ì„¸ìš”")
    print("\nğŸ’¡ Jupyter Labì´ ì‘ë™í•˜ë©´ pyspark_setup.ipynbë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!")
    
    return spark, df

if __name__ == "__main__":
    spark, df = main() 